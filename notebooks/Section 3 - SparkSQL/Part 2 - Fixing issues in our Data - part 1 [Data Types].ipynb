{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Fixing issues in our Data - part 1\n",
    "\n",
    "... introduction  \n",
    "... mention sql functions  \n",
    "To achieve this we are going to be using some functions from [the pyspark.sql.functions module](https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html#module-pyspark.sql.functions).  \n",
    "*__TIP:__ Take your time to study the `functions` module, as it contains a lot of useful functions.*\n",
    "\n",
    "... that functions need to be explicitly imported, as they are otherwise not available for use\n",
    "\n",
    "... Same as in part 1, we are going to be using the `ratings.csv` as provided in the MovieLens data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession and Settings\n",
    "Before we continue, set up a SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MovieLens_Tutorial\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we have to define some settings to ensure proper operations.\n",
    " - `RATINGS_CSV_LOCATION` is used to tell our Spark Application where to find the ratings.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the ratings.csv  file\n",
    "RATINGS_CSV_LOCATION = \"/home/jovyan/data/ml-latest-small/ratings.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "Before proceeding, ensure that the CSV data is loaded to a proper `df` object (output of part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Type safe loading of ratings.csv file\n",
    "df = spark.read.csv(\n",
    "    path=RATINGS_CSV_LOCATION,\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    quote='\"',\n",
    "    encoding=\"UTF-8\",\n",
    "    schema=\"userId INT, movieId INT, rating DOUBLE, timestamp INT\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unix Timestamps\n",
    "\n",
    "The `timestamp` column in the DataFrame we previously loaded does not seem to be parsed well yet, as it is not very human readable. A value in this timestamp column looks something like this `964982703`.\n",
    "\n",
    "`README.txt` belonging to the MovieLens data says this about the timestamp column:\n",
    "> Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n",
    "\n",
    "These type of timestamps are called [__POSIX time__ or __UNIX Epoch time__](https://en.wikipedia.org/wiki/Unix_time).\n",
    "\n",
    "\n",
    "## Converting Timestamps using SQL Functions\n",
    "Let's set out to convert this Unix timestamp to a proper, human-readable, `TimeStampType()` value using some of the functions available to use in the functions module.\n",
    "\n",
    "We are going to need the following functions to allow us to do this:\n",
    "\n",
    " - We need [the `col()` function](https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html#pyspark.sql.functions.col):\n",
    " This allows us to reference columns by their name.\n",
    " - We need [the `from_unixtime()` function](https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html#pyspark.sql.functions.from_unixtime):\n",
    " This allows us to convert the UNIX Epoch time to a `StringType`.  \n",
    " - We need [the `to_timestamp()` function](https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html#pyspark.sql.functions.to_timestamp):\n",
    " This allow us to convert from `StringType` to a `TimestampType`. \n",
    " \n",
    "So let's import these into our Spark Application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_unixtime, to_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to want to format our Timestamp in a certain format, we have to tell the system what format we want to use.  \n",
    "*__TIP__: A good reference for possible formats for dates and times can be found here: https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default ISO8601 format for dates\n",
    "DATEFORMAT = \"yyyy-MM-dd'T'HH:mm:ssZ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall plan of approach:\n",
    "1. Rename original `timestamp` column to one named `timestamp_unix` using [`.withColumnRenamed` DataFrame operation](https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumnRenamed)\n",
    "2. We regenerate a `timestamp` column using [`.withColumn` DataFrame operation](https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumn), and fill it using the `from_unixtime` to transform the unix timestamp to a string with the given format \n",
    "   *We defined the format of our timestamp in the `DATEFORMAT` variable previously*\n",
    "3. We overwrite the `timestamp` column again using [`.withColumn` DataFrame operation](https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumn), converting the previously generated string to a TimeStampType using `to_timestamp`\n",
    "\n",
    "Let's start with the first two steps and see what our data then looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename timestamp to timestamp_unix\n",
    "df = df.withColumnRenamed(\"timestamp\", \"timestamp_unix\")\n",
    "\n",
    "# Converting UNIX time to String with the given DATEFORMAT\n",
    "df = df.withColumn(\"timestamp\", from_unixtime(col(\"timestamp_unix\"), DATEFORMAT))\n",
    "\n",
    "# Displaying results of the transformation\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our timestamp has now been converted to a string with a nice format  \n",
    "> `|-- timestamp: string (nullable = true)`\n",
    "\n",
    "Lets convert it to a TimestampType with the `to_timestamp` function and see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp string with the given DATEFORMAT to TimeStampType()\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"), DATEFORMAT))\n",
    "\n",
    "# Let's see our output\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our data looks the way it should. The unixtime is correctly converted to a timestamp value.  \n",
    "In the schema you can now see `|-- timestamp: timestamp (nullable = true)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping columns\n",
    "\n",
    "For good measure, let's drop the `timestamp_unix` column since we will not be needing it anymore.  \n",
    "We can do this using [the `.drop` method](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.drop) on our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant column\n",
    "df = df.drop(col('timestamp_unix'))\n",
    "\n",
    "# Let's see our output\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining Operations\n",
    "... a note about chaining operations  \n",
    "... so far every operation has been on a newline, this is not only way of doing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chained operation, from loading to cleansing, all in a single operation\n",
    "df = spark.read.csv(\n",
    "    RATINGS_CSV_LOCATION,\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    quote='\"',\n",
    "    schema=\"userId INT, movieId INT, rating DOUBLE, timestamp INT\",\n",
    ").withColumn(\"timestamp\", to_timestamp(from_unixtime(col(\"timestamp\"))))\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we've learned so far:\n",
    "- how to convert between strings and timestamps using the SparkSQL `functions` module\n",
    "- that `functions` need to be explicitly imported, as they are otherwise not available for use\n",
    "- how to rename, add, manipulate, and drop columns using the `.withColumn`, `withColumnRenamed`, and `.drop()` methods\n",
    "- how operations can be efficiently chained...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the spark application, frees up resources\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "end of part 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
